<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[parquet转csv]]></title>
    <url>%2F2019%2F04%2F01%2Fparquet%E8%BD%ACcsv%2F</url>
    <content type="text"><![CDATA[parquet是什么Apache Parquet 是一种 列式存储 格式可用于 Hadoop 生态系统中的任何组件，无论是数据处理框架，数据模型，或者编程语言。Parquet 文件格式包含几个支持数据仓库风格操作的功能 : 列式存储设计 - 仅仅从数据文件或者表中读取一小部分数据时查询可以检测和执行计算所有值中的一个列。 灵活的压缩选项 - 数据能够使用几种编码器压缩。可以将不同的数据文件压缩成不同的格式。 新颖的编码方案 - 相同的，相似的或者相关数据值的序列可以存储在硬盘和内存。The encoding schemes provide an extra level of space savings beyond overall compression for each data file。 大的文件 - Parquet 数据文件是被设计用于优化查询大量数据，单个文件大小在 MB 甚至 GB 以上。 在实际的数据挖掘工作中，可能会有把parquet文件转为csv后本地验证或者实验的需求。 如何把parquet转为csv前期准备需要安装pyarrow库：1conda install pyarrow or1pip install pyarrow 处理转换python文件如下：1234567891011121314151617181920212223242526272829303132333435363738import osimport pandas as pdimport pyarrow.parquet as pqdef read_pyarrow(path, use_threads=1): return pq.read_table(path, use_threads=use_threads).to_pandas()def get_file_list(file_dir='.'): L = [] for root, _, files in os.walk(file_dir): for file in files: if os.path.splitext(file)[1] == '.parquet': L.append(os.path.join(root, file)) return Ldef get_csv(file_list): init_flag = 0 for f in file_list: print('The current handling file is:\n', f) if init_flag == 0: init_df = read_pyarrow(f) init_flag = 1 else: t_df = read_pyarrow(f) init_df = pd.concat([init_df, t_df]) return init_dfpath = 'JoinPredict-20190401055632-SLOT_0-29358'file_list = get_file_list(path)df = get_csv(file_list)df.to_csv('./parquet_data.csv', sep=',', index=False, mode='w', line_terminator='\n', encoding='utf-8') 笔者本身的需求便是要把一个文件夹内的多个parquet文件转换为一个总的csv文件。如需要分别转换，修改源码即可。 githubgithub源码 参考链接Parquet 文件]]></content>
      <categories>
        <category>工具环境</category>
      </categories>
      <tags>
        <tag>parquet2csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴槿惠与文明6]]></title>
    <url>%2F2019%2F03%2F29%2F%E6%9C%B4%E6%A7%BF%E6%83%A0%E4%B8%8E%E6%96%87%E6%98%8E6%2F</url>
    <content type="text"><![CDATA[朴槿惠应该是凉透了。 我现在说这话颇有点高声疾呼“大清亡了！”的感觉，这不怪我，我确实是准备动身去韩国才又想到她了。 机缘巧合，我是亲身经历过朴大妈被下台的某些场景。大约是二零一六年秋天的某个周六，在异国多少想到点热闹的地方以证明自己还是个社会活物，我便到了首尔以热闹著称的明洞地区。然而这个热闹有点出乎我的意料。 从地铁出来，基本上每隔五分钟即可看到一队人马从我身边抢过。他们或头戴不同的布条或戴不同帽子，举着各式各样的旗帜，手拿很多标语，多数均有着“朴槿惠退勤”的韩语（这韩语翻译是笔者按着汉语拼音拼的，或许本意不是如此）。他们到了明洞的周边的一个宽敞的地方，便坐了下来。那边也早已经搭好了台子，有大屏幕，大扩音器。台上有若干位在大声演讲，先前的一位白人男性讲英语，后有一位女士翻译成韩语，我基本都没怎么听明白，但还是能感受到激愤。大屏幕上偶尔会出现朴槿惠的头像，再加上“善意？”的反问文字。 我起初自然是觉得有点意思，便驻足观看，但语言问题，实在是无法获取这群人除了想让朴槿惠下台之外更多的信息，或许本来的信息也就这个吧，看着看着便走神了。 突然，我想到了一个事情，便是李敖在北大金刚怒目的演讲。李敖在演讲过程中举了几个坦克的例子说全世界统治者都是王八蛋。他一口的京片子，说得台上的人的表情像是在当众便秘。他也像是感觉到了空气中尴尬的气氛，一个劲儿地打趣，但好像无济于事。我想到这便有点担心，慢慢地往回退，毕竟到此间只是出差工作，周末顺便参观以充实一下娱乐活动，出点什么问题不太值当。 我便退回了住处，后面便再没去过，一来二去，也渐渐忘了。在我没有去现场做吃瓜群众后的一段时间，朴槿惠便下台了。 偶尔，也玩玩游戏。有个游戏，叫Sid Meier’s Civilization VI，玩家一般就简称文明6。这游戏就是模拟各大主要文明从原始社会到现代社会的变迁与发展，玩起来一般时间较长，有如进了烂柯山，不知岁月。 这游戏在每个城市都会有个城市人民幸福感的数据，你得建好城市基础设施，建好娱乐设施才不会让他下降。如果一个城市的幸福感下降到-5以下，便有可能在城市周边出现暴动。这种暴动令我这个上帝视角统治者甚是心烦。 心烦是心烦，问题还是要解决。做为上帝视角统治者，我并没有和暴动者谈判，解释，而是直接在附近城市快速建立坦克和机械化部队，追着叛乱者打，把叛乱者打光为止，然后再在叛乱城市建一些娱乐设施，修改文明政策如国家认同降低反叛情绪等等。现在回想起来，我是结结实实地做了一回王八蛋。但如果再玩这游戏，这王八蛋我还是会做下去。 我不想说清楚这游戏到底和朴槿惠有什么关系，只是依稀想到大概是中学的课本有提到“国家是阶级统治的工具”。那么有的工具，便是没经过你同意，便使用某种方法对你进行敲打。而有的工具，便是一些事经由你同意，你可以投票选择是皮鞭、大棒还是其他物事对你敲打，当然，你还可以投票选择由谁来向你敲打。从被敲打的人的角度来说，我实在没有看出来二者有什么差异，都是一个房间的sadist，做任意一种sadist做出优越感出来都让我感到这个世界的黑色幽默。 这次再次去韩国，朴槿惠早已下台了，文在寅在位。我重开一把文明6，可能会考虑一下叛乱市民的感受，不做王八蛋了，做一个温柔的王八蛋。一切都好像不一样了，不过又好像什么都没变。 二零一八年六月三日]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow使用]]></title>
    <url>%2F2019%2F03%2F29%2Fairflow%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Airflow基本概念Airflow中的相关概念如下： Operators：Airflow定义的一系列算子/操作符，更直接的理解就是python class。不同的Operator类实现了具体的功能，比如： BashOperator：可以执行用户指定的一个Bash命令 PythonOperator：可以执行用户指定的一个python函数 EmailOperator：可以进行邮件发送 Sensor：感知器/触发器，可以定义触发条件和动作，在条件满足时执行某个动作。Airflow提供了更具体的Sensor，比如FileSensor，DatabaseSensor等 DAG(Directed Acyclic Graph): 字面意有向无环图。是执行任务流的图，在此集合中可以定义任务的依赖关系，另外这个DAG是由python实现，存放在$AIRFLOW_HOME路径下的dags文件夹下，可以看成是一个对象，在使用时需要进行实例化。DAG中包含task。 task：任务，Operators的具体实例。使用步骤 根据实际需要，使用不同的Operator 传入具体的参数，定义一系列的Tasks 定义Tasks间的关系，形成一个DAG 调度DAG运行，每个Task会行成一个Instance 使用命令行或者Web UI进行查看和管理 DAG代码示例下面是一个官方的DAG的python文件示例，为了方便理解，笔者加入了中文注释：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455"""Code that goes along with the Airflow tutorial located at:https://github.com/apache/airflow/blob/master/airflow/example_dags/tutorial.py"""from airflow import DAGfrom airflow.operators.bash_operator import BashOperatorfrom datetime import datetime, timedelta# 以下为DAG的默认参数，这些参数会传给每个operatordefault_args = &#123; 'owner': 'airflow', # 任务的owner,建议用unix user的用户名 'depends_on_past': False, # 当设置为True时，任务实例将按顺序运行，同时依赖于前一个任务的调度成功 'start_date': datetime(2019, 4, 1), # 'email': ['airflow@example.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, # 在任务报失败前应执行的重试次数 'retry_delay': timedelta(minutes=5), # 重试间的延时 # 'queue': 'bash_queue', # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1),&#125;# 传参建立dag，其中'luke_airflow'即为dag_id，是dag的唯一标识，schedule_interval为执行频率dag = DAG('luke_airflow', default_args=default_args, schedule_interval=timedelta(days=1))# t1, t2 and t3 are examples of tasks created by instantiating operatorst1 = BashOperator( task_id='print_date', # task_id，任务的唯一标识 bash_command='date', # 执行的bash命令 dag=dag)t2 = BashOperator( task_id='sleep', bash_command='sleep 5', retries=3, dag=dag)templated_command = """ &#123;% for i in range(5) %&#125; echo "&#123;&#123; ds &#125;&#125;" echo "&#123;&#123; macros.ds_add(ds, 7)&#125;&#125;" echo "&#123;&#123; params.my_param &#125;&#125;" &#123;% endfor %&#125;"""t3 = BashOperator( task_id='templated', bash_command=templated_command, params=&#123;'my_param': 'Parameter I passed in'&#125;, dag=dag)t2.set_upstream(t1) # 设置t1为t2的前置任务，参数中可以为task的列表。t3.set_upstream(t1) # 设置t1为t3的前置任务 上面这个文件实质是一个配置文件(像未实例化的对象)，这个脚本不能用于不同文件之间通信，如果要交叉通信，需要用Xcom 另外在task中的传参有如下优先级： BashOperator指定的参数； 如果没有，则用传入的default_args； 如果依然没有，则用Operator的默认参数。测试解析脚本把上述的文件放到和airflow.cfg同目录下的dags文件夹下，执行。1python3 luke_airflow.py 确保没有报错。 验证脚本让我们运行一些命令来进一步验证这个脚本。12345678# 打印所有激活的dag列表，可以看到luke_airflow的dag在其中airflow list_dags# 打印指定id的dag中任务，这里为"luke_airflow"，可以看到dag中的任务airflow list_tasks luke_airflow# 打印dag中任务树，可以看到dag中任务层级图。airflow list_tasks luke_airflow --tree 执行测试1airflow test luke_airflow templated 2019-03-03 后面跟的日期为模拟执行日期，可以看到执行结果。 其他以上一些操作也可在建立的airflow 网站上用webUI进行操作。 参考链接 airflow 官网 使用 Airflow 替代你的 crontab Apache Airflow]]></content>
      <categories>
        <category>工程实现</category>
      </categories>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WSL自定义安装路径]]></title>
    <url>%2F2019%2F03%2F26%2FWSL%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[WSL是什么Windows Subsystem for Linux（简称WSL）是一个为在Windows 10上能够原生运行Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，目标是使纯正的Ubuntu映像能下载和解压到用户的本地计算机，并且映像内的工具和实用工具能在此子系统上原生运行。 在windows 10专业版上面可以使用。可以免去虚拟机安装的麻烦。 WSL有什么问题一个很麻烦的问题是WSL默认在windows商店里面安装，默认安装到系统盘，且后续的根文件系统均在系统盘中，对于系统盘资源较紧张者比较麻烦。 如何解决针对于以上问题，有如下步骤解决: 下载wsl离线安装包wsl离线安装包下载 在Downloading distros中找到要下载的版本下载.Appx文件。 安装LxRunOfflineLxRunOffline下载 解压放在程序路径，并在系统环境变量中添加： 在cmd中有LxRunOffline命令对应提示即为成功。 用LxRunOffline安装wsl离线安装包首先在powershell中输入 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 来打开Linux子系统功能，也可以在控制面板的启用或关闭windows功能勾选打开。 然后使用LxRunOffline 1LxRunOffline i -n &lt;安装名称&gt; -d &lt;安装路径&gt; -f &lt;安装文件&gt; 其中安装名称可以自定义，安装路径为自定义安装路径，安装文件为上一步解压后的文件中的install.tar.gz的路径，回车后等待安装完成。示例如下： 备注若系统中安装不止一个WSL,则可以通过LxRunOffline sd -n &lt;安装名称&gt;设置默认启动系统，然后在cmd中输入wsl启动系统。若忘记安装名称，可通过LxRunOffline list命令查看。 参考链接 百度百科WSL介绍 自定义安装路径安装WSL]]></content>
      <categories>
        <category>工具环境</category>
      </categories>
      <tags>
        <tag>WSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我为什么写作]]></title>
    <url>%2F2019%2F03%2F25%2F%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%86%99%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[我对写东西的爱好大约是从高中时起，高中的一位老师谬赞过几次，本着“士为知己者死，女为悦己者容”的心理，每每做语文的作文，甚至有些兴奋。 那时我也想过做一个作家。 何为作家？大众所认识的，便是读第四声的“作”，意为“写作”。而我认为“作家”应当念做“作”（第一声）家。“作”就是要整天没事找事，没事也要弄出点事才能消停的。 不作，是做不了作家的。所谓“为赋新词强说愁”就是这个情况。 然而我到现在，却是越来越不作了，只是偶尔还有写点东西的习惯。每每写点东西，有人问起目的来，我实在是答不上来。 似乎中国传统上总习惯于“文以载道”或者强行“文以载道”，一篇文章，不总结个什么中心思想出来 ，总觉得是不成功的。名人写的一句话，不代表个什么玩意儿，那肯定是咱没理解到位，诸如“两棵枣树”之类。我水平有限，也不爱干些好为人师的事，就喜欢写些东西。 其实大学时我应该是有机会走这条路的。 大概在大一的某个学期，中南大学的一个校刊大概叫什么《桥》的面向校内学生招聘。我兴匆匆地过去，是一个面试，算是我人生中第一个面试。年代较为久远，记忆非常模糊，过程大致如下： 一位小姑娘在右边，一位操着很重湖南口音的男的在左边，两个人轮流在问我一些问题。“你对王蒙了解如何？”那男的道，语气大概和“你知道‘回’字的四样写法吗？”一样。 “王……王蒙？”，我当时应该是懵的，说实在的，我那时真是了解不多（其实“那时”二字可以去掉，我现在仍然了解不多）。接着他又问了一些问题，依稀都与“文学圈”相关的吧，我基本都没答上来。更要亲命的是很多问题因为湖南口音，我甚至在多次请求重述后都没听懂， 大概是看我回答的均不是很好，那男的也有些不悦，“你是来应聘什么的？！” “编……辑吧”，我当时已经产生了自我怀疑。 “啥？你来应聘编辑啊？我还以为你是来搞校对的哦！”那男的说了这么一句话。我实在是不太懂，就算我是要“搞”校对，也没必要一定要懂那些玩意吧。 然而这个面试就是这么结束了，也让我在相当长一段时间内没写什么东西，毕竟我连王蒙都不了解，不配在这个圈子里混，不配写什么东西了。 我的文学梦之路大概也就到这结束了，我希望高中给我作文满分的老师看到这些能够谅解我。 路漫漫其修远兮，吾上上下下求不到锁。 文学梦没了，还是要恰饭的，毕业后便进了职场。 单说“职场”的“场”字，学物理的我是极不喜欢的。“场”字，即代表着这其中有某种规律。在这个“场”中，一来人比较容易给自己设定上限，大家各司其职，久而久之，你都不知道你原本可以更厉害。二来人比较容易懈怠，当一个公司没有“打江山”的氛围，都是“坐江山”的氛围，特别是表面上看上去“江山永固”的时候，人自然懈怠。最后，在这个“场”中，人比较容易放弃思考。当每个人做些事，每个月就能拿到收入，而且这个收入还凑合，人是不太愿意去折腾，去想太多的，所谓“怎么样都是赚钱，怎么样都有钱赚”。 当潮水退去的时候，才知道谁在裸泳。 所以我还是写一些东西，保持思考，尽量去找自己的底裤。 如果非要我给自己这种写的行为下个定义，我想了想，借用之前三五好友一起闲聊聊到“外围女”的定义：“外围女即想进娱乐圈，但进不去，老在外围的女的”，我大概就是个文学圈“外围”吧。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow 从入门到放弃]]></title>
    <url>%2F2019%2F03%2F22%2Fairflow-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83%2F</url>
    <content type="text"><![CDATA[airflow是什么Airflow 被 Airbnb 内部用来创建、监控和调整数据管道。任何工作流都可以在这个使用 Python 编写的平台上运行（目前加入 Apache 基金会孵化器）。 Airflow 允许工作流开发人员轻松创建、维护和周期性地调度运行工作流（即有向无环图或成为DAGs）的工具。在Airbnb中，这些工作流包括了如数据存储、增长分析、Email发送、A/B测试等等这些跨越多部门的用例。这个平台拥有和 Hive、Presto、MySQL、HDFS、Postgres和S3交互的能力，并且提供了钩子使得系统拥有很好地扩展性。除了一个命令行界面，该工具还提供了一个 基于Web的用户界面让您可以可视化管道的依赖关系、监控进度、触发任务等。 在ETL(Extract-Transform-Load)中会经常使用脚本（bash/python）+ crontab来运行数据处理任务，然而这种方案存在以下问题 查看任务执行情况不直观方便，只能登录机器、或者写一个界面/监控 存在依赖关系的任务没办法保证，或者保证的方法成本太高 任务量达到一定量级，任务的管理将极其棘手 而airflow有如下优点： Airbnb开源的工作流管理平台 工作流依赖关系的可视化 日志追踪 用Python编写，易于扩展 开箱即用的ETL调度管理平台 运维/作业管理平台 调度平台设计 在一个机器学习建模的工程项目中，数据的ETL处理非常需要airflow。 airflow怎么得到安装airflow原则上不需要多说，但实际操作的过程中出现一些问题。笔者在windows 10的ubuntu 子系统上用python3安装airflow，最开始就出现了如下问题：1234567891011121314pip3 install apache-airflowCollecting apache-airflow Using cached https://files.pythonhosted.org/packages/e4/06/45fe64a358ae595ac562640ce96a320313ff098eeff88afb3ca8293cb6b9/apache-airflow-1.10.2.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File "&lt;string&gt;", line 1, in &lt;module&gt; File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 429, in &lt;module&gt; do_setup() File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 287, in do_setup verify_gpl_dependency() File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 53, in verify_gpl_dependency raise RuntimeError("By default one of Airflow's dependencies installs a GPL " RuntimeError: By default one of Airflow's dependencies installs a GPL dependency (unidecode). To avoid this dependency set SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install or upgrade Airflow. To force installing the GPL version set AIRFLOW_GPL_UNIDECODE 我们可以看到后面给出相关的解决方法，设置SLUGIFY_USES_TEXT_UNIDECODE=yes，如下：1export SLUGIFY_USES_TEXT_UNIDECODE=yes 再次用pip3 install apache-airflow安装，成功！按照官方步骤执行：123export AIRFLOW_HOME=~/airflowairflow initdbairflow webserver -p 8080 在浏览器地址栏输入http://localhost:8080，得下图： 整个airflow的安装在细节上还会有比较多的小问题，要细心排查。上面如果安装后找不到airflow的命令，则找到airflow的脚本所在路径，加入系统PATH即可。实际项目中路径可能会不一样，需要灵活处理。airflow的使用相关的内容请点击airflow使用 参考链接 airflow github 开源中国airflow首页 记一次自认为成功的技术选型——AIRFLOW]]></content>
      <categories>
        <category>工程实现</category>
      </categories>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
</search>

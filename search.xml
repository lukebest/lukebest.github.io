<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[提升树原理与推导]]></title>
    <url>%2Fposts%2Fe647.html</url>
    <content type="text"><![CDATA[简介 本文主要讲提升树的模型与算法，并会在最后对梯度提升树进行比较与描述。在看XGBoost原理与推导前，可先看本文，对提升树有一个基本了解。 提升树模型 我们先来看一下提升树的模型，提升方法实际采用的仍然是加法模型(即基函数的线性组合)和前向分步算法。当基函数是决策树的时候，这个提升方法就叫提升树。根据上述，我们可以把提升树的模型公式表示出来： \[f_M(x)=\sum_{m=1}^MT(x;\Theta_m) \tag{1}\] 其中，\(T(x;\Theta_m)\)就表示决策树。\(x\)为输入的样本。\(\Theta_m\)为第\(m\)棵树的参数，从回归树原理与推导中来看，类似其叶子节点权重。\(M\)为树的总棵树。 提升树算法 提升树算法也是用前向分步算法，我们设第\(m\)步的模型是 \[f_m(x)=f_{m-1}(x)+T(x;\Theta_m) \tag{2}\] 也就是说模型是在之前训练好的结果上加上当前的决策树的输出。 对于第\(m\)步的最优参数\(\hat{\Theta}_m\)，根据经验风险最小化的策略，有： \[\hat{\Theta}_m=\arg \min_{\Theta_m}\sum_{i=1}^N\mathcal{L}(y_i,f_m(x_i)) \tag{3}\] 我们假定采用平方误差损失，即有： \[\mathcal{L}(y,f_m(x))=(y-f_m(x))^2 \tag{4}\] 我们把(2)式代入(4)式，有： \[\mathcal{L}(y,f_{m-1}(x)+T(x;\Theta_m))=(y-f_{m-1}(x)-T(x;\Theta_m))^2 \tag{5}\] 我们令 \[r=y-f_{m-1}(x) \tag{6}\] 则(5)式可以写成： \[\mathcal{L}(y,f_{m-1}(x)+T(x;\Theta_m))=(r-T(x;\Theta_m))^2 \tag{7}\] 我们把\(r\)叫作当前模型拟合数据的残差(residual)，从(7)式可以看到，要使损失变小，即要让当前的树的输出拟合当前模型的残差。如何去拟合呢？可以参考回归树原理与推导。 梯度提升树 对于上述提升树，我们做了一个假设为损失为平方误差损失，如果推及一般损失函数，每一步的优化并不简单了。针对这个问题，Freidman提出了梯度提升的算法，即利用损失函数的负梯度在当前模型的值来近似(6)式中的残差(如果损失为平方误差损失，那么负梯度就是残差)。即 \[r_{mi}=-[\frac{\partial \mathcal{L(y_i,f(x_i))}}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)} \tag{8}\] 其中\(i\)为样本数，\(m\)为提升迭代第\(m\)轮。与我们比较熟悉的梯度下降的算法思想类似。 参考文献 《统计学习方法》李航 8.4 Greedy function approximation: a gradient boosting machine]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>原理</tag>
        <tag>推导</tag>
        <tag>GBDT</tag>
        <tag>提升树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归树原理与推导]]></title>
    <url>%2Fposts%2F917e.html</url>
    <content type="text"><![CDATA[CART简介 本文主要讲回归树和最小二乘回归树的算法，目的是对决策树做回归有一个认识，并且熟悉经典的最小二乘回归树。我们这里只关注CART的回归树， CART(classification and regressioin tree)是在给定输入随机变量\(X\)条件下输出 随机变量\(Y\)的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”。其中的回归决策树等价于递归地二分每个特征，将输入空间即特征空间切分为有限个单元，并在这些单元上确定预测的回归值。当切分完毕，这些单元就是回归树上的叶子节点，这些单元上的预测值，就是叶子节点的取值。 怎么做 从上述简介可以看出生成一个回归树大致需要两部，第一步，切分特征空间为有限个单元；第二步，在每个切分后的单元上确定代表该单元的回归值。下面先讲如何确定单元取值，再讲如何切分特征空间。 确定单元取值 假设\(X\)与\(Y\)分别为输入和输出变量，并且\(Y\)是连续变量，给定训练数据集： \[D=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\] 其中\(x \in R^d\)，一个回归树对应着输入空间（即特征空间）的一个切分和在切分后的单元上的输出值。我们这里假设输入空间已经被切分为\(M\)个单元\(R_1,R_2,\dots,R_M\)，并且每个单元有一个固定的输出值\(c_m\)，那么回归树的模型就可以写成： \[f(x)=\sum_{m=1}^Mc_mI(x \in R_m) \tag{1}\] 那么如何确定每个\(R_m\)上的取值呢？我们这里的最小二乘回归树用平方误差来表示回归树的单元取值与真实值的误差，即： \[\mathcal{L}=\sum_{x_i \in R_m}(y_i-f(x_i))^2 \tag{2}\] 根据最小二乘法，可以很容易知道单元\(R_m\)上的\(c_m\)的最优值\(\hat{c}_m\)是\(R_m\)的所有样本的输出\(y_i\)的均值，即： \[\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i \tag{3}\] 其中\(N_m\)为落在切分空间\(R_m\)的所有样本数目，\(R_m(j,s)\)意思是切分变量\(x^{(j)}\)(即输入变量的第\(j\)维特征)和对应的切分点\(s\)对应的切后单元空间。到这里就解决了问题：假设切分空间确定后，每个空间所代表的值。 切分特征空间 那么回归树的问题就剩余如何确定输入空间的切分。事实上只要找到切分变量\(x^{(j)}\)(即输入变量的第\(j\)维特征)和对应的切分点\(s\)，然后再针对切分后的子空间递归进行切分直到满足建树停止条件即可。 这里定义切分后的两个区域为\(R_1\)和\(R_2\)，有： \[R_1(j,s)=\{x|x^{(j)}\leq s\}\] \[R_2(j,s)=\{x|x^{(j)}&gt; s\}\] 然后求解如下式寻找最优切分变量\(j\)和最优切分点\(s\)： \[min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2] \tag{4}\] 解释一下(4)式，即选取输入变量\(x\)的第\(j\)维，并扫描切分点\(s\)，当切分的两个子单元的方差之和最小，则为第\(j\)维的最优切分点。遍历所有找到符合(4)式的\(j,s\)，切分为两个子单元，再对子单元按(4)式进行递归切分直到满足停止条件即可。 这里找切分点的方法思想本质还是最小二乘法，与图像二值化当中的Otsu算法有点神似。 参考文献 《统计学习方法》李航 5.5.1]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>原理</tag>
        <tag>推导</tag>
        <tag>最小二乘回归树</tag>
        <tag>回归树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理与推导]]></title>
    <url>%2Fposts%2F933a.html</url>
    <content type="text"><![CDATA[简介 本文不讲如何使用XGBoost也不讲如何调参，主要会讲一下作为GBDT中的一种，XGBoost的原理与相关公式推导。为了循序渐进的理解，读者可先从简单的回归树再到提升树再来看本文。关于回归树与提升树占位链接如下。我们现在直接从XGBoost的目标函数讲起。 回归树原理与推导 提升树原理与推导 XGBoost公式推导 XGBoost的目标函数如下： \[ obj=\sum_{i=1}^n\mathcal{L}(y_i,\hat{y}_i)+\sum \Omega(f_k),f_k\in\mathcal{F} \tag1\] 上面的两项加号前项为在训练集上的损失函数。其中\(y_i\)表示真实值，\(\hat{y}_i\)表示预测值。加号后项为正则项，到后面再看\(\Omega\)这个函数的具体形式。我们现在只需要知道\(\Omega\)的自变量为\(f_k\)，是决策树，而不是向量，所以是没有办法用和导数有关的方法来训练的（像梯度下降等）。 Boosting 何为Boosting，这个可以主要在上面给的提升树文章中去了解。这里大概描述如下： \[\hat{y}_i^{(0)}=0\] \[\hat{y}_i^{(1)}=f_1(x_i)=\hat{y}_i^{(0)}+f_1(x_i)\] \[\hat{y}_i^{(2)}=f_1(x_i)+f_2(x_i)=\hat{y}_i^{(1)}+f_2(x_i)\] 依此类推，每次迭代轮数均比上次迭代好一些，通式如下： \[\hat{y}_i^{(t)}=\sum_{k=1}^tf_k(x_i)=\hat{y}_i^{(t-1)}+f_t(x_i) \tag2\] 上面(2)式即为对于第t轮的boosting公式。我们再看此时的目标函数： \[ obj^{(t)}=\sum_{i=1}^n\mathcal{L}(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^t \Omega(f_k) \tag3\] 其中(3)式\(n\)为样本数，\(t\)为树的棵数，也是迭代的轮数，\(y_i\)为真实值，\(\hat{y}_i\)为第\(t\)轮的预测值 。对于正则化项，又可以写成如下形式： \[ \sum_{i=1}^t \Omega(f_k)=\Omega(f_t)+ \sum_{i=1}^{t-1} \Omega(f_k)\tag4\] (4)式这么写有什么好处呢？因为我们的方法是boosting逐轮计算的，所以当计算第\(t\)轮时，前面\(t-1\)轮事实上是已经计算出来了的。即(4)式的加号后项为常数。所以把(2)(4)式代入(3)式，有如下： \[ obj^{(t)}=\sum_{i=1}^n\mathcal{L}(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+const \tag5\] Taylor展开 在(5)这个式子的基础上，我们就可以做点文章了。我们现在假定经验损失\(\mathcal{L}\)是可以二阶Taylor展开的。把\(f_t(x_i)\)当成无穷小，就得到了如下式： \[ obj^{(t)}\approx\sum_{i=1}^n[\mathcal{L}(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)+const \tag6\] (6)这个式子是比较抽象的，为帮助对\(g_i\)和\(h_i\)的理解，我们把常见的平方损失函数代入(5)可有： \[ obj^{(t)}=\sum_{i=1}^n(y_i-(\hat{y}_i^{(t-1)}+f_t(x_i)))^2+\Omega(f_t)+const\] 展开有： \[ obj^{(t)}=\sum_{i=1}^n[(y_i-\hat{y}_i^{(t-1)})^2+2(\hat{y}_i^{(t-1)}-y_i)f_t(x_i)+f_t^2(x_i)]+\Omega(f_t)+const\] 即： \[ obj^{(t)}=\sum_{i=1}^n[\mathcal{L}(y_i,\hat{y}^{(t-1)})+2(\hat{y}_i^{(t-1)}-y_i)f_t(x_i)+f_t^2(x_i)]+\Omega(f_t)+const\] 套用一下(6)式，即有当损失函数为平方损失时\(g_i=2(\hat{y}_i^{(t-1)}-y_i)\)，\(h_i=2\)。 我们再考察(6)式，其中的\(\mathcal{L}(y_i,\hat{y}_i^{(t-1)})\)意义是\(t-1\)轮的经验损失，在执行第\(t\)轮的时候，这一项其实已经也是一个已知的常数。那么优化目标就可以继续简化如下： \[ obj^{(t)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t) \tag7\] 其中，\(g_i\)和\(h_i\)可根据Taylor公式求得如下： \[\begin{cases} g_i=\partial_{\hat{y}^{(t-1)}}\mathcal{L}(y_i,\hat{y}^{(t-1)})\\ h_i=\partial^2_{\hat{y}^{(t-1)}}\mathcal{L}(y_i,\hat{y}^{(t-1)})\\ \end{cases} \tag8\] 也就是说在给定损失函数的形式，则\(g_i\)和\(h_i\)就可以算出来。 树的权重求解与结构分 我们从(7)式再往下，把\(f_t(x)\)搞清楚。有： \[f_t(x)=w_{q(x)},w\in R^T,q:R^d\to \{1,2,\cdots,T\} \tag9\] \(f_t(x)\)表示第\(t\)棵树，\(x\)为输入特征，其维数为\(d\)，\(f_t(x)\)即为把特征映射到一个树的叶子结点上的一个数（权重），\(f_t(x)\)可以分为\(w\)和\(q\)两部分，其中\(q\)为把特征映射到一个\(T\)个叶子结点的函数，相当于是决策树的结构。\(w\)为把每个叶子结点映射为其权重值。 树的函数搞清了，如果没有搞清楚，可以从本文开头提到的回归树原理与推导和提升树原理与推导再深入学习一下。 我们考察(7)式中的罚项\(\Omega(f_t)\)，这个罚项是用来惩罚树的复杂程度的。根据上面\(f_t(x)\)的描述，我们可以从树的结构与树叶子结点上的权重做罚项，我们定义如下： \[\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2 \tag{10}\] 其中\(T\)为叶子节点个数，\(w\)为叶子节点权重。我们把(10)式加号前项叫L0范数，加号后项为L2范数，\(\gamma\)和\(\lambda\)分别为各自超参数。 我们把(10)式，(9)式都代入(7)式，有： \[ obj^{(t)}=\sum_{i=1}^n[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2 \tag{11}\] 到这里，我们可以看到(7)式中优化事实上可以着重分为两大步，第一步拿到树结构，第二步再计算叶子节点上的权重大小。我们先令： \[I_j=\{i|q(x_i)=j\},j\in \{1,2,\cdots,T\} \tag{12}\] (12)式的意思是定义所有落在第\(j\)个叶子节点的样本为\(I_j\)，即被映射到第\(j\)个叶子节点上的所有样本\(x_i\)的索引。那么对(11)式按树的叶子节点分组重写如下： \[ obj^{(t)}=\sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w^2_j]+\gamma T\tag{13}\] 我们令 \[G_j=\sum_{i\in I_j}g_i \tag{14}\] \[H_j=\sum_{i\in I_j}h_i \tag{15}\] 把(14)(15)代入(13)有： \[ obj^{(t)}=\sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w^2_j]+\gamma T\tag{16}\] 我们假定这时候树的结构已经知道，那么映射到叶子节点\(j\)上面的样本索引就固定了，而损失函数也是已知的，由(8)可有\(g_i\)和\(h_i\)都是已知的，进而\(G_j\)和\(H_j\)则都是已知的。那么，目标函数就变成了求叶子节点个数\(T\)个的关于\(w\)的一元二次函数求和的形式，而且他们两两之间都是独立的。接下来我们对每个\(w\)进行求解，用到的就是高中数学知识了。对于每个这个形式的一元二次函数，可有取极值时的\(w\)： \[argmin_w[Gw+\frac{1}{2}(H+\lambda)w^2]=-\frac{G}{H+\lambda},H&gt;0 \tag{17}\] 这个时候极值为: \[min_w[Gw+\frac{1}{2}(H+\lambda)w^2]=-\frac{G^2}{2(H+\lambda)} \tag{18}\] 即这个时候，目标函数的极值： \[\hat{obj}^{(t)}=-\sum_{j=1}^T\frac{G^2}{2(H+\lambda)}+\gamma T \tag{19}\] 也被称为树的结构分。 到这里，我们解决了一个问题，就是在知道每轮的树的结构\(q\)的前提下，我们能够很快求得每个叶子节点上的最优权重值\(w\)。 如何找到最优的树结构 那么如何获取最优的树结构呢？方式一：遍历所有可能的树结构，计算每种可能的树的结构分，然后再找到最小结构分对应的权重。这种方式可想而知根本无法实际应用。方式二：基于贪心策略，每次分裂都使得分裂后的增益最大。这种方式其实与决策树的树基本生成算法类似，只是增益的定义不一样。我们这里的增益可以用树的结构分来定义，每次树的分裂点使得整体树的结构分下降得最大，定义如下： \[\mathcal{L}_{split}=\frac{1}{2}(\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L^2+G_R^2)}{H_L+H_R+\lambda})-\gamma \tag{20}\] 总结 我们对本文从上到下总结一下。XGBoost首先确定好树的棵数。对于每轮的那棵树，其形成步骤大体可以分为两大块，第一要先确定好树的最优结构，一般会根据树的结构分下降最大的分裂点进行贪心算法构造，当然这里会有一些单棵树的深度或者叶子节点数限制。当树的结构确定好之后，对于每个叶子节点的权重，可以很快地用一元二次函数解法求得。当此轮的树生成完毕后，即可求得每个样本的预测结果，从而进行下一轮迭代。 参考文献 XGBoost: A Scalable Tree Boosting System]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>原理</tag>
        <tag>推导</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
        <tag>提升方法</tag>
        <tag>boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression原理与推导]]></title>
    <url>%2Fposts%2Fd28f.html</url>
    <content type="text"><![CDATA[简介 本文不讲如何使用逻辑回归，主要会讲一下逻辑回归的算法和模型背后的假设。主要分为两部分，一个是逻辑回归公式的推导，其次会讲述一下如何理解sigmoid函数。 逻辑回归推导 首先我们知道逻辑回归的一个sigmoid假设，这个假设背后的由来我们会在后面给出，请先记得这个假设。公式如下： \[ p(y=1|x) = \frac {1} {1+e^{-(\theta^Tx+b)}} \tag1\] 这个假设告诉我们分为正样本的概率为(1)中右式。其中\(x\)为输入的向量，计算的结果与0.5比较，如果大于0.5则为正样本。右式的值域易知为(0,1)。对于负样本，有： \[ p(y=0|x) = 1-p(y=1|x) \tag2\] 综合上述二式可得： \[ p(y|x) = p(y=1|x)^yp(y=0|x)^{1-y} \tag3\] 注意得到上式用到了数学的trick，即下式： \[\begin{cases} p(y=1|x),y=1\\ p(x=0|x),y=0\\ \end{cases} \] 我们继续往下。事实上，从(3)式我们就知道了概率函数\(p(y|x)\)，拿到概率函数，我们可以求其最大似然估计（maximum likelihood estimation，MLE）。 MLE是用来估计概率模型的参数的一种方法，最大似然估计会寻找关于 \(\theta\) 的最可能的值（即，在所有可能的\(\theta\) 取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在\(\theta\)的所有可能取值中寻找一个值使得似然函数取到最大值。这个使可能性最大的\(\widehat{\theta}\)值即称为 \(\theta\)的最大似然估计。由定义，最大似然估计是样本的函数。 如果有n个样本，那么其似然函数为(3)式的n个样本的乘积： \[ likelihood=\prod_{i=1}^np(y_i|x_i) \tag4\] \[ likelihood=\prod_{i=1}^np(y_i=1|x_i)^{y_i}p(y_i=0|x_i)^{1-y_i} \tag5\] 对(5)取log： \[ log\_likelihood=\sum_{i=1}^ny_i*ln(p(y_i=1|x_i))+(1-y_i)*ln(p(y_i=0|x_i)) \tag6\] 于是就变成了对数似然函数(6)，对log_likelihood，我们是希望它越大越好（极大似然估计），如果对其取负，则可以作为损失函数，我们是希望损失越小越好。这里的优化方法，先从简单的说，可用梯度下降进行迭代优化（对数似然函数对应地用梯度上升）。 我们现在把最初始的(1)式代入(6)： \[ log\_likelihood=\sum_{i=1}^ny_i*ln(\frac {1} {1+e^{-(\theta^Tx_i+b)}})+(1-y_i)*ln(1-\frac {1} {1+e^{-(\theta^Tx_i+b)}}) \tag7\] 对于sigmoid函数，求导是非常方便的。即有： \[ f(x)=\frac1{1+e^{-x}} \tag8\] \[ f^\prime(x)=f(x)*(1-f(x)) \tag9\] 那么(7)式对\(\theta\)求梯度有： \[ \nabla_\theta log\_likelihood=\sum_{i=1}^n x_i*(y_i-\frac{1}{1+e^{-(\theta^T x_i+b)}}) \tag{10}\] 其中(10)式中的b可以先忽略（可以与\(\theta\)统一）。 迭代优化方式 BGD(Batch Gradient Descent) 对极大似然函数梯度上升，有： \[ \vec\theta_{i+1}= \vec\theta_{i}+\lambda*\nabla_\theta log\_likelihood \tag{11}\] 把(10)代入，即： \[ \vec\theta_{i+1}= \vec\theta_{i}+\lambda*\sum_{i=1}^n \vec x_i*(y_i-\frac{1}{1+e^{-(\vec\theta^T \vec x_i+b)}}) \tag{12}\] (12)式就是全批量梯度更新方法，其中\(\lambda\)即为一般意义上的学习步长。从(12)式可以看到，要把n个数据样本全部代入后方可进行梯度更新。计算来说是比较繁琐的。而根据监督学习的假设样本数据是依据某种联合概率分布独立同分布产生，那么可有一种一次用一个样本的梯度更新方式，就是SGD(Stochastic Gradient Descent)。 SGD(Stochastic Gradient Descent) 随机梯度更新方式就是在(12)式去掉了求和符号： \[ \vec\theta_{i+1}= \vec\theta_{i}+\lambda*\vec x_i*(y_i-\frac{1}{1+e^{-(\vec\theta^T \vec x_i+b)}}) \tag{13}\] 随机梯度更新方式来一条样本即可做一次更新，通常情况下会引入较大的随机性。BGD和SGD很明显都比较极端，那么就有了一种折中方式Mini-Batch GD Mini-Batch GD(Mini-Batch Gradient Descent) 与BGD类似，只是所有样本的计算换成部分样本计算： \[ \vec\theta_{i+1}= \vec\theta_{i}+\lambda*\sum_{i=1}^m \vec x_i*(y_i-\frac{1}{1+e^{-(\vec\theta^T \vec x_i+b)}}) \tag{14}\] 其中\(m\ll n\)，这样做的好处就是可以综合SGD和BGD。在较快更新速度的同时可以有一定的随机性防止陷入局部最优。以上的梯度更新方式在神经网络的训练中也可以借鉴。 sigmoid函数的由来 我们在最开始有一个假设，一切的出发点为(1)中的假设。那么为什么会有这样一个假设呢？ 我们先看一个一般性的二分类的贝叶斯公式，二类分别为C1和C2： \[ p(C1|x)=\frac{p(x|C1)p(C1)}{p(x|C1)p(C1)+p(x|C2)p(C2)} \tag{15}\] 我们对(15)式右边的分子分母同时除以\(p(x|C1)p(C1)\)（二分类问题可保证其一定不为0），那么就有： \[ p(C1|x)=\frac1{1+\frac{p(x|C2)p(C2)}{p(x|C1)p(C1)}} \tag{16}\] 我们可以令： \[a=ln\frac{p(x|C1)p(C1)}{p(x|C2)p(C2)} \tag{17}\] 那么把(17)代入(16)，即有： \[ p(C1|x)=\frac1{1+e^{-a}} \tag{18}\] 上面的(18)式就是我们最初的sigmoid函数。我们再看一下(17)式，用人话把\(ln\)的对象描述如下：分类为C1的x出现的概率比上分类为C2的x出现的概率。 广义指数分布函数族 虽然(18)式看着和我们最开始的假设很像，但细心的读者可以看到最开始假设的\(a\)是\(\theta\)和\(x\)的线性函数。而(17)式目前看不出来像。 我们先看一下广义指数分布函数族的概念，对于广义指数分布函数族，有： \[ p(x|\lambda_k)=h(x)g(\lambda_k)e^{\lambda_k^Tu(x)} \tag{19}\] 把(19)代入(17)，有： \[a=ln\frac{h(x)g(\lambda_1)e^{\lambda_1^Tu(x)}p(C1)}{h(x)g(\lambda_2)e^{\lambda_2^Tu(x)}p(C2)} \tag{20}\] \(h(x)\)可以约掉，\(g(\lambda_i)\)是常数项可有： \[a=(\lambda_1-\lambda_2)^Tu(x)+ln(\frac{g(\lambda_1)}{g(\lambda_2)})+ln(\frac{p(C1)}{p(C2)}) \tag{21}\] 即： \[a=(\lambda_1-\lambda_2)^Tu(x)+const \tag{22}\] 上面就告诉我们，当\(p(x|C1)\)和\(p(x|C2)\)满足广义指数分布，并且是其中一种特例（\(u(x)=x\)），就可以满足\(a\)是\(\theta\)和\(x\)的线性函数，其中\((\lambda_1-\lambda_2)=\theta\)。 其他广义指数分布函数 这里给出结论，感兴趣者可作详细推导：满足\(a\)是\(\theta\)和\(x\)的线性函数的广义指数分布还有如下： Gaussian分布 Binomial分布 Poisson分布 Bernoulli分布 可以看到逻辑回归的适用范围是比较广的。 参考链接 最大似然估计 Exponential family]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>逻辑回归</tag>
        <tag>logistic regression</tag>
        <tag>SGD</tag>
        <tag>BGD</tag>
        <tag>梯度下降</tag>
        <tag>原理</tag>
        <tag>推导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[parquet转csv]]></title>
    <url>%2Fposts%2Fcd96.html</url>
    <content type="text"><![CDATA[parquet是什么 Apache Parquet 是一种 列式存储 格式可用于 Hadoop 生态系统中的任何组件，无论是数据处理框架，数据模型，或者编程语言。Parquet 文件格式包含几个支持数据仓库风格操作的功能 : - 列式存储设计 - 仅仅从数据文件或者表中读取一小部分数据时查询可以检测和执行计算所有值中的一个列。 - 灵活的压缩选项 - 数据能够使用几种编码器压缩。可以将不同的数据文件压缩成不同的格式。 - 新颖的编码方案 - 相同的，相似的或者相关数据值的序列可以存储在硬盘和内存。The encoding schemes provide an extra level of space savings beyond overall compression for each data file。 - 大的文件 - Parquet 数据文件是被设计用于优化查询大量数据，单个文件大小在 MB 甚至 GB 以上。 在实际的数据挖掘工作中，可能会有把parquet文件转为csv后本地验证或者实验的需求。 如何把parquet转为csv 前期准备 需要安装pyarrow库： 1conda install pyarrow or 1pip install pyarrow 处理转换 python文件如下： 1234567891011121314151617181920212223242526272829303132333435363738import osimport pandas as pdimport pyarrow.parquet as pqdef read_pyarrow(path, use_threads=1): return pq.read_table(path, use_threads=use_threads).to_pandas()def get_file_list(file_dir='.'): L = [] for root, _, files in os.walk(file_dir): for file in files: if os.path.splitext(file)[1] == '.parquet': L.append(os.path.join(root, file)) return Ldef get_csv(file_list): init_flag = 0 for f in file_list: print('The current handling file is:\n', f) if init_flag == 0: init_df = read_pyarrow(f) init_flag = 1 else: t_df = read_pyarrow(f) init_df = pd.concat([init_df, t_df]) return init_dfpath = 'JoinPredict-20190401055632-SLOT_0-29358'file_list = get_file_list(path)df = get_csv(file_list)df.to_csv('./parquet_data.csv', sep=',', index=False, mode='w', line_terminator='\n', encoding='utf-8') 笔者本身的需求便是要把一个文件夹内的多个parquet文件转换为一个总的csv文件。如需要分别转换，修改源码即可。 github github源码 参考链接 Parquet 文件]]></content>
      <categories>
        <category>工具环境</category>
      </categories>
      <tags>
        <tag>parquet2csv</tag>
        <tag>数据处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴槿惠与文明6]]></title>
    <url>%2Fposts%2F69e5.html</url>
    <content type="text"><![CDATA[朴槿惠应该是凉透了。 我现在说这话颇有点高声疾呼“大清亡了！”的感觉，这不怪我，我确实是准备动身去韩国才又想到她了。 机缘巧合，我是亲身经历过朴大妈被下台的某些场景。大约是二零一六年秋天的某个周六，在异国多少想到点热闹的地方以证明自己还是个社会活物，我便到了首尔以热闹著称的明洞地区。 然而这个热闹有点出乎我的意料。 从地铁出来，基本上每隔五分钟即可看到一队人马从我身边抢过。他们或头戴不同的布条或戴不同帽子，举着各式各样的旗帜，手拿很多标语，多数均有着“朴槿惠退勤”的韩语（这韩语翻译是笔者按着汉语拼音拼的，或许本意不是如此）。他们到了明洞的周边的一个宽敞的地方，便坐了下来。那边也早已经搭好了台子，有大屏幕，大扩音器。台上有若干位在大声演讲，先前的一位白人男性讲英语，后有一位女士翻译成韩语，我基本都没怎么听明白，但还是能感受到激愤。大屏幕上偶尔会出现朴槿惠的头像，再加上“善意？”的反问文字。 我起初自然是觉得有点意思，便驻足观看，但语言问题，实在是无法获取这群人除了想让朴槿惠下台之外更多的信息，或许本来的信息也就这个吧，看着看着便走神了。 突然，我想到了一个事情，便是李敖在北大金刚怒目的演讲。李敖在演讲过程中举了几个坦克的例子说全世界统治者都是王八蛋。他一口的京片子，说得台上的人的表情像是在当众便秘。他也像是感觉到了空气中尴尬的气氛，一个劲儿地打趣，但好像无济于事。我想到这便有点担心，慢慢地往回退，毕竟到此间只是出差工作，周末顺便参观以充实一下娱乐活动，出点什么问题不太值当。 我便退回了住处，后面便再没去过，一来二去，也渐渐忘了。在我没有去现场做吃瓜群众后的一段时间，朴槿惠便下台了。 偶尔，也玩玩游戏。有个游戏，叫Sid Meier’s Civilization VI，玩家一般就简称文明6。这游戏就是模拟各大主要文明从原始社会到现代社会的变迁与发展，玩起来一般时间较长，有如进了烂柯山，不知岁月。 这游戏在每个城市都会有个城市人民幸福感的数据，你得建好城市基础设施，建好娱乐设施才不会让他下降。如果一个城市的幸福感下降到-5以下，便有可能在城市周边出现暴动。这种暴动令我这个上帝视角统治者甚是心烦。 心烦是心烦，问题还是要解决。做为上帝视角统治者，我并没有和暴动者谈判，解释，而是直接在附近城市快速建立坦克和机械化部队，追着叛乱者打，把叛乱者打光为止，然后再在叛乱城市建一些娱乐设施，修改文明政策如国家认同降低反叛情绪等等。 现在回想起来，我是结结实实地做了一回王八蛋。但如果再玩这游戏，这王八蛋我还是会做下去。 我不想说清楚这游戏到底和朴槿惠有什么关系，只是依稀想到大概是中学的课本有提到“国家是阶级统治的工具”。那么有的工具，便是没经过你同意，便使用某种方法对你进行敲打。而有的工具，便是一些事经由你同意，你可以投票选择是皮鞭、大棒还是其他物事对你敲打，当然，你还可以投票选择由谁来向你敲打。从被敲打的人的角度来说，我实在没有看出来二者有什么差异，都是一个房间的sadist，做任意一种sadist做出优越感出来都让我感到这个世界的黑色幽默。 这次再次去韩国，朴槿惠早已下台了，文在寅在位。我重开一把文明6，可能会考虑一下叛乱市民的感受，不做王八蛋了，做一个温柔的王八蛋。一切都好像不一样了，不过又好像什么都没变。 二零一八年六月三日]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>心明录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow使用]]></title>
    <url>%2Fposts%2F6a60.html</url>
    <content type="text"><![CDATA[Airflow基本概念 Airflow中的相关概念如下： - Operators：Airflow定义的一系列算子/操作符，更直接的理解就是python class。不同的Operator类实现了具体的功能，比如： - BashOperator：可以执行用户指定的一个Bash命令 - PythonOperator：可以执行用户指定的一个python函数 - EmailOperator：可以进行邮件发送 - Sensor：感知器/触发器，可以定义触发条件和动作，在条件满足时执行某个动作。Airflow提供了更具体的Sensor，比如FileSensor，DatabaseSensor等 - DAG(Directed Acyclic Graph): 字面意有向无环图。是执行任务流的图，在此集合中可以定义任务的依赖关系，另外这个DAG是由python实现，存放在$AIRFLOW_HOME路径下的dags文件夹下，可以看成是一个对象，在使用时需要进行实例化。DAG中包含task。 - task：任务，Operators的具体实例。 使用步骤 根据实际需要，使用不同的Operator 传入具体的参数，定义一系列的Tasks 定义Tasks间的关系，形成一个DAG 调度DAG运行，每个Task会行成一个Instance 使用命令行或者Web UI进行查看和管理 DAG代码示例 下面是一个官方的DAG的python文件示例，为了方便理解，笔者加入了中文注释： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455"""Code that goes along with the Airflow tutorial located at:https://github.com/apache/airflow/blob/master/airflow/example_dags/tutorial.py"""from airflow import DAGfrom airflow.operators.bash_operator import BashOperatorfrom datetime import datetime, timedelta# 以下为DAG的默认参数，这些参数会传给每个operatordefault_args = &#123; 'owner': 'airflow', # 任务的owner,建议用unix user的用户名 'depends_on_past': False, # 当设置为True时，任务实例将按顺序运行，同时依赖于前一个任务的调度成功 'start_date': datetime(2019, 4, 1), # 'email': ['airflow@example.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, # 在任务报失败前应执行的重试次数 'retry_delay': timedelta(minutes=5), # 重试间的延时 # 'queue': 'bash_queue', # 'pool': 'backfill', # 'priority_weight': 10, # 'end_date': datetime(2016, 1, 1),&#125;# 传参建立dag，其中'luke_airflow'即为dag_id，是dag的唯一标识，schedule_interval为执行频率dag = DAG('luke_airflow', default_args=default_args, schedule_interval=timedelta(days=1))# t1, t2 and t3 are examples of tasks created by instantiating operatorst1 = BashOperator( task_id='print_date', # task_id，任务的唯一标识 bash_command='date', # 执行的bash命令 dag=dag)t2 = BashOperator( task_id='sleep', bash_command='sleep 5', retries=3, dag=dag)templated_command = """ &#123;% for i in range(5) %&#125; echo "&#123;&#123; ds &#125;&#125;" echo "&#123;&#123; macros.ds_add(ds, 7)&#125;&#125;" echo "&#123;&#123; params.my_param &#125;&#125;" &#123;% endfor %&#125;"""t3 = BashOperator( task_id='templated', bash_command=templated_command, params=&#123;'my_param': 'Parameter I passed in'&#125;, dag=dag)t2.set_upstream(t1) # 设置t1为t2的前置任务，参数中可以为task的列表。t3.set_upstream(t1) # 设置t1为t3的前置任务 上面这个文件实质是一个配置文件(像未实例化的对象)，这个脚本不能用于不同文件之间通信，如果要交叉通信，需要用Xcom 另外在task中的传参有如下优先级： 1. BashOperator指定的参数； 2. 如果没有，则用传入的default_args； 3. 如果依然没有，则用Operator的默认参数。 测试 解析脚本 把上述的文件放到和airflow.cfg同目录下的dags文件夹下，执行。 1python3 luke_airflow.py 确保没有报错。 验证脚本 让我们运行一些命令来进一步验证这个脚本。 12345678# 打印所有激活的dag列表，可以看到luke_airflow的dag在其中airflow list_dags# 打印指定id的dag中任务，这里为"luke_airflow"，可以看到dag中的任务airflow list_tasks luke_airflow# 打印dag中任务树，可以看到dag中任务层级图。airflow list_tasks luke_airflow --tree 执行测试 1airflow test luke_airflow templated 2019-03-03 后面跟的日期为模拟执行日期，可以看到执行结果。 其他 以上一些操作也可在建立的airflow 网站上用webUI进行操作。 参考链接 airflow 官网 使用 Airflow 替代你的 crontab Apache Airflow]]></content>
      <categories>
        <category>工程实现</category>
      </categories>
      <tags>
        <tag>airflow</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WSL自定义安装路径]]></title>
    <url>%2Fposts%2Fbb32.html</url>
    <content type="text"><![CDATA[WSL是什么 Windows Subsystem for Linux（简称WSL）是一个为在Windows 10上能够原生运行Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，目标是使纯正的Ubuntu映像能下载和解压到用户的本地计算机，并且映像内的工具和实用工具能在此子系统上原生运行。 在windows 10专业版上面可以使用。可以免去虚拟机安装的麻烦。 WSL有什么问题 一个很麻烦的问题是WSL默认在windows商店里面安装，默认安装到系统盘，且后续的根文件系统均在系统盘中，对于系统盘资源较紧张者比较麻烦。 如何解决 针对于以上问题，有如下步骤解决: 下载wsl离线安装包 wsl离线安装包下载 在Downloading distros中找到要下载的版本下载.Appx文件。 安装LxRunOffline LxRunOffline下载 解压放在程序路径，并在系统环境变量中添加： 在cmd中有LxRunOffline命令对应提示即为成功。 用LxRunOffline安装wsl离线安装包 首先在powershell中输入 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 来打开Linux子系统功能，也可以在控制面板的启用或关闭windows功能勾选打开。 然后使用LxRunOffline 1LxRunOffline i -n &lt;安装名称&gt; -d &lt;安装路径&gt; -f &lt;安装文件&gt; 其中安装名称可以自定义，安装路径为自定义安装路径，安装文件为上一步解压后的文件中的install.tar.gz的路径，回车后等待安装完成。示例如下： 备注 若系统中安装不止一个WSL,则可以通过LxRunOffline sd -n &lt;安装名称&gt;设置默认启动系统，然后在cmd中输入wsl启动系统。若忘记安装名称，可通过LxRunOffline list命令查看。 参考链接 百度百科WSL介绍 自定义安装路径安装WSL]]></content>
      <categories>
        <category>工具环境</category>
      </categories>
      <tags>
        <tag>WSL</tag>
        <tag>Linux</tag>
        <tag>Win10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我为什么写作]]></title>
    <url>%2Fposts%2F35d9.html</url>
    <content type="text"><![CDATA[我对写东西的爱好大约是从高中时起，高中的一位老师谬赞过几次，本着“士为知己者死，女为悦己者容”的心理，每每做语文的作文，甚至有些兴奋。 那时我也想过做一个作家。 何为作家？大众所认识的，便是读第四声的“作”，意为“写作”。而我认为“作家”应当念做“作”（第一声）家。“作”就是要整天没事找事，没事也要弄出点事才能消停的。 不作，是做不了作家的。所谓“为赋新词强说愁”就是这个情况。 然而我到现在，却是越来越不作了，只是偶尔还有写点东西的习惯。每每写点东西，有人问起目的来，我实在是答不上来。 似乎中国传统上总习惯于“文以载道”或者强行“文以载道”，一篇文章，不总结个什么中心思想出来 ，总觉得是不成功的。名人写的一句话，不代表个什么玩意儿，那肯定是咱没理解到位，诸如“两棵枣树”之类。我水平有限，也不爱干些好为人师的事，就喜欢写些东西。 其实大学时我应该是有机会走这条路的。 大概在大一的某个学期，中南大学的一个校刊大概叫什么《桥》的面向校内学生招聘。我兴匆匆地过去，是一个面试，算是我人生中第一个面试。年代较为久远，记忆非常模糊，过程大致如下： 一位小姑娘在右边，一位操着很重湖南口音的男的在左边，两个人轮流在问我一些问题。 “你对王蒙了解如何？”那男的道，语气大概和“你知道‘回’字的四样写法吗？”一样。 “王……王蒙？”，我当时应该是懵的，说实在的，我那时真是了解不多（其实“那时”二字可以去掉，我现在仍然了解不多）。接着他又问了一些问题，依稀都与“文学圈”相关的吧，我基本都没答上来。更要亲命的是很多问题因为湖南口音，我甚至在多次请求重述后都没听懂， 大概是看我回答的均不是很好，那男的也有些不悦，“你是来应聘什么的？！” “编……辑吧”，我当时已经产生了自我怀疑。 “啥？你来应聘编辑啊？我还以为你是来搞校对的哦！”那男的说了这么一句话。我实在是不太懂，就算我是要“搞”校对，也没必要一定要懂那些玩意吧。 然而这个面试就是这么结束了，也让我在相当长一段时间内没写什么东西，毕竟我连王蒙都不了解，不配在这个圈子里混，不配写什么东西了。 我的文学梦之路大概也就到这结束了，我希望高中给我作文满分的老师看到这些能够谅解我。 路漫漫其修远兮，吾上上下下求不到锁。 文学梦没了，还是要恰饭的，毕业后便进了职场。 单说“职场”的“场”字，学物理的我是极不喜欢的。“场”字，即代表着这其中有某种规律。 在这个“场”中，一来人比较容易给自己设定上限，大家各司其职，久而久之，你都不知道你原本可以更厉害。二来人比较容易懈怠，当一个公司没有“打江山”的氛围，都是“坐江山”的氛围，特别是表面上看上去“江山永固”的时候，人自然懈怠。最后，在这个“场”中，人比较容易放弃思考。当每个人做些事，每个月就能拿到收入，而且这个收入还凑合，人是不太愿意去折腾，去想太多的，所谓“怎么样都是赚钱，怎么样都有钱赚”。 当潮水退去的时候，才知道谁在裸泳。 所以我还是写一些东西，保持思考，尽量去找自己的底裤。 如果非要我给自己这种写的行为下个定义，我想了想，借用之前三五好友一起闲聊聊到“外围女”的定义：“外围女即想进娱乐圈，但进不去，老在外围的女的”，我大概就是个文学圈“外围”吧。]]></content>
      <categories>
        <category>杂文</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>心明录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow 从入门到放弃]]></title>
    <url>%2Fposts%2F8439.html</url>
    <content type="text"><![CDATA[airflow是什么 Airflow 被 Airbnb 内部用来创建、监控和调整数据管道。任何工作流都可以在这个使用 Python 编写的平台上运行（目前加入 Apache 基金会孵化器）。 Airflow 允许工作流开发人员轻松创建、维护和周期性地调度运行工作流（即有向无环图或成为DAGs）的工具。在Airbnb中，这些工作流包括了如数据存储、增长分析、Email发送、A/B测试等等这些跨越多部门的用例。这个平台拥有和 Hive、Presto、MySQL、HDFS、Postgres和S3交互的能力，并且提供了钩子使得系统拥有很好地扩展性。除了一个命令行界面，该工具还提供了一个 基于Web的用户界面让您可以可视化管道的依赖关系、监控进度、触发任务等。 在ETL(Extract-Transform-Load)中会经常使用脚本（bash/python）+ crontab来运行数据处理任务，然而这种方案存在以下问题 - 查看任务执行情况不直观方便，只能登录机器、或者写一个界面/监控 - 存在依赖关系的任务没办法保证，或者保证的方法成本太高 - 任务量达到一定量级，任务的管理将极其棘手 而airflow有如下优点： - Airbnb开源的工作流管理平台 - 工作流依赖关系的可视化 - 日志追踪 - 用Python编写，易于扩展 - 开箱即用的ETL调度管理平台 - 运维/作业管理平台 - 调度平台设计 在一个机器学习建模的工程项目中，数据的ETL处理非常需要airflow。 airflow怎么得到 安装airflow原则上不需要多说，但实际操作的过程中出现一些问题。笔者在windows 10的ubuntu 子系统上用python3安装airflow，最开始就出现了如下问题： 1234567891011121314pip3 install apache-airflowCollecting apache-airflow Using cached https://files.pythonhosted.org/packages/e4/06/45fe64a358ae595ac562640ce96a320313ff098eeff88afb3ca8293cb6b9/apache-airflow-1.10.2.tar.gz Complete output from command python setup.py egg_info: Traceback (most recent call last): File "&lt;string&gt;", line 1, in &lt;module&gt; File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 429, in &lt;module&gt; do_setup() File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 287, in do_setup verify_gpl_dependency() File "/tmp/pip-build-465wd_ew/apache-airflow/setup.py", line 53, in verify_gpl_dependency raise RuntimeError("By default one of Airflow's dependencies installs a GPL " RuntimeError: By default one of Airflow's dependencies installs a GPL dependency (unidecode). To avoid this dependency set SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you install or upgrade Airflow. To force installing the GPL version set AIRFLOW_GPL_UNIDECODE 我们可以看到后面给出相关的解决方法，设置SLUGIFY_USES_TEXT_UNIDECODE=yes，如下： 1export SLUGIFY_USES_TEXT_UNIDECODE=yes 再次用pip3 install apache-airflow安装，成功！ 按照官方步骤执行： 123export AIRFLOW_HOME=~/airflowairflow initdbairflow webserver -p 8080 在浏览器地址栏输入http://localhost:8080，得下图： 整个airflow的安装在细节上还会有比较多的小问题，要细心排查。上面如果安装后找不到airflow的命令，则找到airflow的脚本所在路径，加入系统PATH即可。 实际项目中路径可能会不一样，需要灵活处理。 airflow的使用相关的内容请点击airflow使用 参考链接 airflow github 开源中国airflow首页 记一次自认为成功的技术选型——AIRFLOW]]></content>
      <categories>
        <category>工程实现</category>
      </categories>
      <tags>
        <tag>airflow</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
</search>
